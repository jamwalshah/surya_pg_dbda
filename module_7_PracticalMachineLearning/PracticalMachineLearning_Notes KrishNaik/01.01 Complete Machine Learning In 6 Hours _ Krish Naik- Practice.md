# 01.01 Complete Machine Learning In 6 Hours | Krish Naik- Practice

## Types of Analytics

- To make decisions based on some data, we have following ways:

1. Predictive Analytics
    - helps to identify trends, correlations and causation within datasets
    - e.g. in healthcare, to forecast regions which will experience a rise in flu cases
2. Prescriptive Analytics
    - predicts likely outcomes
    - e.g. makes decision recommendations like when a tire will wear out and need to be replaced
3. Diagnostic Analytics
    - helps pinpoint the reason an event occurred
    - e.g. Manufacturers can analyze a failed components on an assembly line and figure out the reason behind its failure
4. Descriptive Analytics
    - evaluates the qualities and quantities of a dataset
    - e.g. a content streaming provider might use descriptive statistics to understand how many subscribers he's lost or gained over a given period of time and what kind of content is being watched

## Data Science Lifecycle

1. Identify the Problem
    - to identify a problem or opportunity
2. Data Mining
    - to extract the data relevant to that problem or opportunity from large datasets
3. Data Cleaning
    - to fix errors & redundancies
4. Data Exploration
    - Data Exploration analysis is to try to make sense of that data
5. Feature Engineering
    - to use domain knowledge to extract details from the data
6. Predictive Modeling
    - to use data to predict or forecast future outcomes and behaviors
7. Data Visualization
    - to represent the data with graphical tools such as charts and animations

## Agenda

1. Introduction to ML (AI vs. ML vs. DL vs. DS)
2. Supervised ML and Unsupervised ML
3. Linear Regression (Maths & Geometric Intuition)
4. R²  & Adjusted R²
5. Ridge and Lasso Regression

## AI vs. ML vs. DL vs. DS

### AI (Artificial Intelligence)

- Imagine the entire universe, it can be called as AI
- When talking about AI, it means Artificial Intelligence
- You're creating an AI application if you'reworking in these roles
  - Machine Learning Developer
  - Deep Learning Developer
  - Vision Developer
  - Data Scientist
  - AI Engineer
- To define Artificial Intelligence (AI), you can say that it is a process where we create some kind of application in which it will be able to do its task without any human intervention
- A person need not monitor an AI application, it will be able to take decisions, perform its tasks, and do many other things
- E.g.
  - Netflix has an AI model
    - Suppose you watch an action movie for some time, then the AI work that is implements here is called recommendation. When you are continuously watching action movies, then automatically the AI model of Netflix will make sure that it will give us recommendation on action movies
    - If you watch comedy movie, it will give us recommendations for comedy movies
    - This recommendation system understands your behavior and it is being able to do its task without asking you anything
  - Amazon also has an AI model
    - If you buy an iPhone, then it may recommend you headphones
    - Such kund of AI model is integrated in Amazon website that will recommend similar things
  - YouTube Ads
    - The Ads that you see when you watch a video on YouTube, those are also recommended using an AI engine that is included in YouTube website which really plays for a business driven goal
  - Self Driving Cars
    - Tesla has self driving cars, based on the road conditions, it is able to drive it automatically with the help of an AI application integrated with the car itself
- It is the business driven things that we basically do with the help of AI

### ML (Machine Learning)

- It is a subset of AI
- It provides statstools to analyze, visualize the data and do predictions and forecasting
- You'll se a lot of Machine Learning Algorithms, and internally those Machine Learning Algorithms uses statstools, because when we work with data statistics it is definitely very much important
- Machine Learning (ML) is a subset of Artificial Intelligence (AI)

![ML-Machine-Learning](../../content/ML-Machine-Learning.png)

### DL (Deep Learning)

- Deep Learning (DL) is a subset of Machine Learning (ML), which is a subset of Artificial Intelligence (AI)
- In 1950-60s, Scientists thought that can we make machine learnlike how human beings learn, so for that particular purpose, Deep Learning came into existence
- Here, the plan is to mimic Human Brain to implement & learn something

![DL-Deep-Learning](../../content/DL-Deep-Learning.png)

- For this, you use Multi-Layered Nerual Networks, and this Multi-Layered Neural Networks will basically help you to train the machines or applications that we're trying to create
- With the help of Deep Learning, we're able to solve such complex use cases

### DS (Data Science)

- If I come to Data Science, a Data Scientist is a part of every thing
- If you tell yourself as a Data Scientist and tomorrow you're given a business use case and situation comes that you probably have to solve the use case with the help of Machine Learning (ML) Algorithms or Deep Learning (DL) Algorithms, again the final goal is to create an Artificial (AI) Application
- You can't tell that you're a Data Scientist and you'll work only in Machine Learning (ML) or only in Deep Learning (DL)

## Machine Learning & Deep Learning

- For problem statements that we solve, the majority of busniess problems will fall majorly under two sections
  
1. Supervised Machine Learning
    1. Regression Problem
    2. Classification Problem
2. Unsupervised Machine Learning
    1. Clustering Problem
    2. Dimensionality Reduction Problem

> Reinforcement Learning can be considered as another type of Machine Learning

### 1. Supervised Machine Learning

- Consider a dataset which has

|Age    |Weight |
|:--    |:--    |
|24     |62     |
|25     |63     |
|21     |72     |
|27     |62     |

- Let's say that the task is to take this data and create a model which takes a new age and it should be able to give the output of weight
- First of all, we'll train this model with this data
- This model is also called `Hypothesis`

- There are two things that are important

1. Independent Feature
    - The inputs based on which the model is being trained are independent features
    - Here the `Age` is the independent Feature
2. Dependent Feature
    - Whatever will be predicted by the model as output is called Dependent Features
    - Here the `Weight` is the dependent feature

- `Weight` is dependent on `Age`, so when `Age` changes, `Weight` also changes
- Here, the model is fed with inputs and outputs and then model predicts based on its learning

#### 1. Regression Problem

- Suppose I take same example of `Age` and `Weight` as

|Age    |Weight |
|:--    |:--    |
|24     |72     |
|23     |71     |
|25     |71.5   |

- Here, Weight is the output variable which is the dependent feature
- Whenever we try to find output, we have it as a continuous variable

> If, it has a continuous variable, then it becomes a Regression Problem statement

- Suppose we're using this data set and we're populating it with the help of a scatter plot
  - Then, in order to solve this problem using linear regression, we'll try to draw/find a straight line which has an equation `y = mx + c`
  - With the help of this equation, we'll try to find out the predicted points
  - In this way, we solve a regression problem by finding out the predicted points

> In a regression problem, your output will always be a continuous variable

![Regression-Problem](../../content/Regression-Problem.png)

#### 2. Classification Problem

- Suppose I have a dataset

|No of Study hours  |No of play hours   |No of Sleep hours  |Output (Pass/Fail) |
|:--                |:--                |:--                |:--                |
|-                  |-                  |-                  |Pass               |
|-                  |-                  |-                  |Fail               |
|-                  |-                  |-                  |Pass               |
|-                  |-                  |-                  |Fail               |
|-                  |-                  |-                  |Pass               |

> Whenever you have a fixed number of categories, then it becomes a classification problem statement

- Here, in the above dataset, we have only two possible outputs, so it becomes a `binary classification problem`
- When you have more than two different categories, then it becomes a `Multi-Class Classification Problem`

### 2. Unsupervised Machine Learning

- There is no output feature/variable, machine learns from the unlabelled data

#### 1. Clustering

- Lets say that my dataset is

|Salary |Age    |
|:--    |:--    |
|50000  |24     |
|20000  |25     |
|25000  |23     |

- Now, in this scenario, we don't have any output variable/dependent variable

> The kind of assumptions that we can make from this dataset is called clustering where we'll make clusters</br>
> Clustering means that based on the the data, we'll try to find out similar groups

- Let's say I'm going to do *Customer Segmentation*
  - So, as per clustering, I'll try to find out similar groups of customers
  - Each of the groups of people will be referred as clusters
  - Each cluster will specify some information, for example which may be
    - `Cluster 1`: These people are very yound and were also able to get amazing salary
    - `Cluster 2`: These people have more age and have good salary
    - `Cluster 3`: These people does not have as much salary as per their age, might be considered as Middle-class people
  - Here, we're grouping as per clusters based on salary and age
  - There is no output feature/variable, so it comes under Unsupervised learning

> Understand that, Clustering is not a classification problem, it is a grouping problem

![Unsupervised-Clustering](../../content/Unsupervised-Clustering.png)

- Use case for clustering is
  - Customer Segmentation
    - Suppose, my company launches **product1** to target **rich people** for it, and **product2** is launched for **middle-class people**
    - So, my company can identify the groups as *rich people* or *middle-class people*, to target the ads for respective groups
    - This activity is called `Customer Segmentation`
    - Based on this customer Segmentation, we can later apply any kind of regression or classification

![Unsupervised-Clustering-UseCase](../../content/Unsupervised-Clustering-UseCase.png)

#### 2. Dimensionality Reduction

- Suppose, we have 1000 features/dimensions, can we reduce it to lower dimensions say 100 features/dimensions
- It is possible to reduce the dimensions with the help of Dimensionality Reduction Algorithms, such as `PCA`, `LDA`
- The idea is to reduce the count of features, in the cases where the count of features of is huge such that it can be reduced into a data set with count of features being few, so that machine can be trained while maintaining the sanity of the data set

## 1. Supervised Machine Learning Algorithms

- Supervised Machine Learning Algorithms include
    1. Linear Regression
    2. Ridge & Lasso Regression
    3. Logistic Regression
    4. Decision Tree
    5. Adaboost
    6. Random Forest
    7. Gradient Boosting
    8. Xg Boost
    9. Naive Bayes
    10. SVN
    11. KNN

### 1. Linear Regression

- It has very simple Problem Statement haing two kinds of variables, independent variables and dependent variables
- Suppose I have two feaatures
    1. X : Age
    2. Y : Weight
- Based on these two features, I have some data points which are present here
- In Linear Regression, we try to create a model with the help of a training dataset, and then predict output upon taking the inputs
- This model is a kind of `Hypothesis Testing` Model, which takes the new Age and gives the outputs of the Weights, and then with the help of performance metrics, we verify whether this model is performing well or not
- In short, we try to find a best fit line which will actually help us to do the prediction, such that if I provide it a new age it should be able to predict an output

![Linear-Regression-Graph](../../content/Linear-Regression-Graph.png)

- From this graph/diagram, I can say that `y` is a linear function of `x`, i.e. `f(x)`
- When ever we say linear regression, it basically means that we're going to create a staright line
- Although we can create non-linear/non-straight/curved lines too using other algorithms, but linear regression means to create a straight line

#### Equation of a Linear Regression

- Linear regression is represented by the equation of a line which is
  - y     = mx + c , where
    - 'm' is the slope/gradient/coefficient
    - 'c' is the y-intercept that lies on the line
- This equation of line/linear regression is also represented by
  - y     = β₀ + β₁x
  - ![Hypothesis-{h_{\theta}(x) = \theta_0 + \theta_1x} (https://latex.codecogs.com/svg.image?{h_{\theta}(x)=\theta_0&plus;\theta_1x})](../../content/Hypothesis-{h_{_theta}(x)=_theta_0+theta_1x}.svg)
    - it is Linear Hypothesis or linear regression model equation where
      - 'y' or ![Predicted-DataPoint-{h_{\theta}(x)} (https://latex.codecogs.com/svg.image?{h_{\theta}(x)})](../../content/Predicted-DataPoint-{h_{_theta}(x)}.svg)
        - hypothesis function, also known as model
        - predicts the output (dependent variable) based on the input variable 'x' (independent variable)
      - 'β₀' or 'θ₀'
        - y-intercept of the linear model
        - shifts the line up or down
      - 'β₁' or 'θ₁'
        - slope of the line or coefficient
        - determines the angle or steepness of the line
      - 'x'
        - input variable
        - represents the features or independent variable
- As per Andrew NG, the equation of a straight line is
  - y = mx⁽ⁱ⁾ + c
  - y = β₀ + β₁x⁽ⁱ⁾
  - ![Hypothesis-i-{h_{\theta}(x^{(i)}) = \theta_0 + \theta_1x^{(i)}} (https://latex.codecogs.com/svg.image?{h_{\theta}(x^{(i)})=\theta_0&plus;\theta_1x^{(i)}})](../../content/Hypothesis-i-{h_{_theta}(x^{(i)})=_theta_0+_theta_1x^{(i)}}.svg)
- To understand what is 'θ₀' and 'θ₁', lets say we have a problem statement, where we have some data points and we're trying to create a best fit line
  - the best fit line is given by the equation ![Hypothesis-{h_{\theta}(x) = \theta_0 + \theta_1x} (https://latex.codecogs.com/svg.image?{h_{\theta}(x)=\theta_0&plus;\theta_1x})](../../content/Hypothesis-{h_{_theta}(x)=_theta_0+theta_1x}.svg)
  
  - ![Equation-of-a-Straight-Line](../../content/Equation-of-a-Straight-Line.png)
  
  - 'θ₀' is the intercept, which means when x = 0, then ![Predicted-DataPoint-{h_{\theta}(x)} (https://latex.codecogs.com/svg.image?{h_{\theta}(x)})](../../content/Predicted-DataPoint-{h_{_theta}(x)}.svg) = θ₀
    - The intercept basically indicates that at what point the best fit line meets the y-axis
  - 'θ₁' is the slope or coefficient
    - slope indicates the movement of y-axis coordinate when a point moves for one-unit of x-axis along the best fit line
  - 'xᵢ' denote the various data points along the best fit line

#### Aim of Linear Regression

- The aim of the Linear Regression is to find the best fit line in such a way that the distance between the data points that we have and the predicted data points should be minumum
- Suppose, we're trying to find a best fit line, we have to ensure that the sum of all the distances between each actual point and its respective predicted data point should be minumum
  - Only when the sum of all the differences of actual and predicted data points is minimum, then we can say that it is the best fit line of linear regression

- ![Sum-of-Differences-of-Actual-and-Predicted-Datapoints-be-Minumum](../../content/Sum-of-Differences-of-Actual-and-Predicted-Datapoints-be-Minumum.png)

#### Cost Function

- To find the best fit line, we can't go like plotting multiple lines along the available data points and then select the best fit line amongst all the possible lines
- Instead, We need to start at one point and we should lead towards finding the best fit line
- To move towards finding the best fit line after plotting one data point, we use `Cost Function`
- The hypothesis is ![Hypothesis-{h_{\theta}(x) = \theta_0 + \theta_1x} (https://latex.codecogs.com/svg.image?{h_{\theta}(x)=\theta_0&plus;\theta_1x})](../../content/Hypothesis-{h_{_theta}(x)=_theta_0+theta_1x}.svg)
  - 'θ₀' and 'θ₁' are the weights that needs to be updated
- The Cost Function is used to find out the best fit line such that there is minimal distance between the data points and predicted data points, and cost function is implemented using the below equation which is the cost function equation
  - ![Cost-Function-{J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)})^2} (https://latex.codecogs.com/svg.image?{J(\theta_0,\theta_1)=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2})](../../content/Cost-Function-{J(_theta_0,_theta_1)=_frac{1}{2m}_sum_{i=1}^{m}(h_{_theta}(x^{(i)})%20-%20y^{(i)})^2}.svg) , where
    - ![Predicted-DataPoints-{h_{\theta}(x^{(i)})} (https://latex.codecogs.com/svg.image?{h_{\theta}(x^{(i)})})](../../content/Predicted-DataPoints-{h_{_theta}(x^{(i)})}.svg)
      - represents the predicted data points for respective input values of 'x'
    - 'y⁽ⁱ⁾'
      - represents the actual data points for respective input values of 'x'
    - ![Squared-Distance-Predicted-Actual-{(h_{\theta}(x^{(i)}) - y^{(i)})^2} (https://latex.codecogs.com/svg.image?{(h_{\theta}(x^{(i)})-y^{(i)})^2})](../../content/Squared-Distance-Predicted-Actual-{(h_{_theta}(x^{(i)})-y^{(i)})^2}.svg)
      - represents the square of the distance between the predicted data points and the actual data points
    - ![Sum-Squared-Distances-Predicted-Actual-{\sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)})^2} (https://latex.codecogs.com/svg.image?{\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2})](../../content/Sum-Squared-Distances-Predicted-Actual-{_sum_{i=1}^{m}(h_{_theta}(x^{(i)})-y^{(i)})^2}.svg)
      - represents the sum of the squares of all the distances between the predicted data points and their respective actual data points
    - 'm'
      - represents the count of the data points
    - ![One-By-m-{{\frac{1}{m}}} (https://latex.codecogs.com/svg.image?{\frac{1}{m}})](../../content/One-By-m-{{_frac{1}{m}}}.svg)
      - used with sum of squares of the distances to give the average
      - sum/count gives the average
    - ![One-By-Two-{{\frac{1}{2}}} (https://latex.codecogs.com/svg.image?{\frac{1}{2}})](../../content/One-By-Two-{{_frac{1}{m}}}.svg)
      - used to simplify the equation for the partial derivation purpose, so that the while updating the weights 'θ₀' and 'θ₁', we try to find the partial derivative of the Cost Function and it becomes easier to find the partial derivative of the equation
      - the partial derivative of x² is given by ![Partial-Derivative-x-squared-{\frac{\partial (x^2)}{\partial x} = 2x} (https://latex.codecogs.com/svg.image?\frac{\partial(x^2)}{\partial&space;x}=2x)](../../content/Partial-Derivative-x-squared-{_frac{_partial(x^2)}{_partialx}=2x}.svg)
      - this above partial derivative of xⁿ is given by the formula ![Partial-Derivative-x-raised-n-{{\frac{\partial(x^n)}{\partial{x}} = {nx^{n-1}}} (https://latex.codecogs.com/svg.image?{{\frac{\partial(x^n)}{\partial{x}}={nx^{n-1}}})](../../content/Partial-Derivative-x-raised-n-{{_frac{_partial(x^n)}{_partial{x}}={nx^{n-1}}}.svg)
      - when we find the partial derivative of J(θ₀, θ₁), it becomes easier to solve it when the equation is divided by 2, so it is again a function in the partial derivative form
  - The cost function is ![Cost-Function-{J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)})^2} (https://latex.codecogs.com/svg.image?{J(\theta_0,\theta_1)=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2})](../../content/Cost-Function-{J(_theta_0,_theta_1)=_frac{1}{2m}_sum_{i=1}^{m}(h_{_theta}(x^{(i)})%20-%20y^{(i)})^2}.svg)
    - it can also be represented in the derivative form as ![Partial-Derivative-Cost-Function-{{\frac{\partial{J(\theta_0,\theta_1)}}{\partial{x}}={2J(\theta_0,\theta_1)=\frac{1}{m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)})^2}} (https://latex.codecogs.com/svg.image?{{\frac{\partial{J(\theta_0,\theta_1)}}{\partial{x}}={2J(\theta_0,\theta_1)=\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2}})](../../content/Partial-Derivative-Cost-Function-{{_frac{_partial{J(_theta_0,_theta_1)}}{_partial{x}}={2J(_theta_0,_theta_1)=_frac{1}{m}_sum_{i=1}^{m}(h_{_theta}(x^{(i)})-y^{(i)})^2}}.svg)
    - The `Cost function` may also be referred as `Squared Error Function`, because distance between the predicted data points and actual data points doesn't come out as a negative value in the sum function

#### What we need to solve in Linear Regression

- We need to minimize the Cost Function, *min. J(θ₀, θ₁)*
- To minimize the Cost Function, we adjust the parameters 'θ₀' and 'θ₁'
- Let's compare with two different things
  1. Hypothesis Testing
  2. with respect to Cost Function

##### Hypothesis Testing & associated Cost Function

- The hypothesis is
  - ![Hypothesis-{h_{\theta}(x) = \theta_0 + \theta_1x} (https://latex.codecogs.com/svg.image?{h_{\theta}(x)=\theta_0&plus;\theta_1x})](../../content/Hypothesis-{h_{_theta}(x)=_theta_0+theta_1x}.svg)

1. If 'θ₀' = 0

    - It means that the best fit line passes through origin
    - when 'θ₀' = 0, then
      - ![Hypothesis-{h_{\theta}(x) = \theta_0 + \theta_1x} (https://latex.codecogs.com/svg.image?{h_{\theta}(x)=\theta_0&plus;\theta_1x})](../../content/Hypothesis-{h_{_theta}(x)=_theta_0+theta_1x}.svg) becomes
      - ![Hypothesis-Theta0-is-0-{h_{\theta}(x) = \theta_1x} (https://latex.codecogs.com/svg.image?{h_{\theta}(x)=\theta_1x})](../../content/Hypothesis-Theta0-is-0-{h_{_theta}(x)=_theta_1x}.svg)
    - ![Hypothesis-Theta0-is-zero-Intercept-Through-Origin](../../content/Hypothesis-Theta0-is-zero-Intercept-Through-Origin.png)
    - Lets consider, I have three actual data points `(1, 1)`, `(2, 2)` and `(3, 3)`
    - Now, keeping the y-intercept 'θ₀' = 0, and changing the value of slope/coefficient 'θ₁'
      1. if 'θ₁' = 1
          - the predicted data points would be `(1, 1)`, `(2, 2)` and `(3, 3)`

          - ![Cost-Function-Theta0-0-Theta1-1-Graph](../../content/Cost-Function-Theta0-0-Theta1-1-Graph.png)
          - ![Cost-Function-Theta0-0-Theta1-1-{J(\theta_1)=\frac{1}{2\times3}\sum_{i=1}^{3}(h_\theta(x^{(i)})-y^{(i)})^2} (https://latex.codecogs.com/svg.image?{J(\theta_1)=\frac{1}{2\times3}\sum_{i=1}^{3}(h_\theta(x^{(i)})-y^{(i)})^2})](../../content/Cost-Function-Theta0-0-Theta1-1-{J(_theta_1)=_frac{1}{2_times3}_sum_{i=1}^{3}(h__theta(x^{(i)})-y^{(i)})^2}.svg)
          - ![Cost-Function-Theta0-0-Theta1-1-2-{=\frac{1}{2\times3}[(1-1)^2+(2-2)^2+(3-3)^2]}(https://latex.codecogs.com/svg.image?{=\frac{1}{2\times3}[(1-1)^2&plus;(2-2)^2&plus;(3-3)^2]})](../../content/Cost-Function-Theta0-0-Theta1-1-2-{=_frac{1}{2_times3}[(1-1)^2+(2-2)^2+(3-3)^2]}.svg)

          - = 0
          - the graph for Cost Function when 'θ₁' = 1 will be plotted as ![Cost-Function-Graph-Theta1-1](../../content/Cost-Function-Graph-Theta1-1.png)
      2. if 'θ₁' = 0.5
          - the predicted data points would be `(1, 0.5)`, `(2, 1)` and `(3, 1.5)`

          - ![Cost-Function-Theta0-0-Theta1-0.5-Graph](../../content/Cost-Function-Theta0-0-Theta1-0.5-Graph.png)
          - ![Cost-Function-Theta0-0-Theta1-0.5-{J(\theta_1)=\frac{1}{2\times3}\sum_{i=1}^{3}(h_\theta(x^{(i)})-y^{(i)})^2} (https://latex.codecogs.com/svg.image?{J(\theta_1)=\frac{1}{2\times3}\sum_{i=1}^{3}(h_\theta(x^{(i)})-y^{(i)})^2})](../../content/Cost-Function-Theta0-0-Theta1-0.5-{J(_theta_1)=_frac{1}{2_times3}_sum_{i=1}^{3}(h__theta(x^{(i)})-y^{(i)})^2}.svg)
          - ![Cost-Function-Theta0-0-Theta1-0.5-2-{=\frac{1}{2\times3}[(0.5-1)^2+(1-2)^2+(1.5-3)^2]}(https://latex.codecogs.com/svg.image?{=\frac{1}{2\times3}[(0.5-1)^2&plus;(1-2)^2&plus;(1.5-3)^2]})](../../content/Cost-Function-Theta0-0-Theta1-0.5-2-{=_frac{1}{2_times3}[(0.5-1)^2+(1-2)^2+(1.5-3)^2]}.svg)
          - ![Cost-Function-Theta0-0-Theta1-0.5-3-{=\frac{1}{2\times3}[0.25+1+2.25]\approx0.58}(https://latex.codecogs.com/svg.image?{=\frac{1}{2\times3}[0.25&plus;1&plus;2.25]\approx0.58})](../../content/Cost-Function-Theta0-0-Theta1-0.5-3-{=_frac{1}{2_times3}[0.25+1+2.25]_approx0.58}.svg)

          - the graph for Cost Function when 'θ₁' = 1 will be plotted as ![Cost-Function-Graph-Theta1-0.5](../../content/Cost-Function-Graph-Theta1-0.5.png)
          - Here, Slope has decreased

      3. if 'θ₁' = 0
          - the predicted data points would be `(1, 0)`, `(2, 0)` and `(3, 0)`

          - ![Cost-Function-Theta0-0-Theta1-0-Graph](../../content/Cost-Function-Theta0-0-Theta1-0-Graph.png)
          - ![Cost-Function-Theta0-0-Theta1-0-{J(\theta_1)=\frac{1}{2\times3}\sum_{i=1}^{3}(h_\theta(x^{(i)})-y^{(i)})^2} (https://latex.codecogs.com/svg.image?{J(\theta_1)=\frac{1}{2\times3}\sum_{i=1}^{3}(h_\theta(x^{(i)})-y^{(i)})^2})](../../content/Cost-Function-Theta0-0-Theta1-0-{J(_theta_1)=_frac{1}{2_times3}_sum_{i=1}^{3}(h__theta(x^{(i)})-y^{(i)})^2}.svg)
          - ![Cost-Function-Theta0-0-Theta1-0-2-{=\frac{1}{2\times3}[(0-1)^2+(0-2)^2+(0-3)^2]} (https://latex.codecogs.com/svg.image?{=\frac{1}{2\times3}[(0-1)^2&plus;(0-2)^2&plus;(0-3)^2]})](../../content/Cost-Function-Theta0-0-Theta1-0-2-{=_frac{1}{2_times3}[(0-1)^2+(0-2)^2+(0-3)^2]}.svg)
          - ![Cost-Function-Theta0-0-Theta1-0-3-{=\frac{1}{6}[1+4+9]\approx2.3} (https://latex.codecogs.com/svg.image?{=\frac{1}{6}[1&plus;4&plus;9]\approx2.3})](../../content/Cost-Function-Theta0-0-Theta1-0-3-{=_frac{1}{6}[1+2+3]_approx2.3}.svg)

          - the graph for Cost Function when 'θ₁' = 0 will be plotted as ![Cost-Function-Graph-Theta1-0](../../content/Cost-Function-Graph-Theta1-0.png)

      4. if 'θ₁' = 0.4
          - the predicted data points would be `(1, 0.4)`, `(2, 0.8)` and `(3, 1.2)`

          - ![Cost-Function-Theta0-0-Theta1-0.4-Graph](../../content/Cost-Function-Theta0-0-Theta1-0.4-Graph.png)

          - Similary, when 'θ₁' = 0.4, then J(θ₁) ≈ 5.04
      - Now, if we keep plotting the Cost function against its 'θ₁' value by changing its value, and join all the points, we'll get a curve, which is called `Gradient Descent`

      - ![Cost-Function-Gradient-Descent-Curve](../../content/Cost-Function-Gradient-Descent-Curve.png)

      - This Gradient Descent plays a very crucial role in making sure that you get the right 'θ₁' or light slope value
      - The most suitable point is the lowest value in the curve, which is called `Global Minima`
      - THis Global Minima corresponds to the best fit line, because the distance between the actual data points and the predicted data points is very less

##### Moving towards Global Minima

- We start with one actual data point and its predicted data point, and then we move towards minimizing the difference between the actual data points and the predicted data points to find the Global Minima
- To find the Global Minima, we use a `Convergance Algorithm`
- If we start with one data point, we need to update the value of 'θ₁', instead of using different 'θ₁' values

> Convergance ALgorithm says that we need to repeat until convergance

- Here, we'll update the 'θⱼ' value repeatedly until er reach the Global Minima

## 2. Unsupervised Machine Learning Algorithms

- Unsupervised Machine Learning Algorithms include
    1. K-Means
    2. DB Scan
    3. Hierearchical Clustering
    4. K-Nearest Neighbor Clustering
    5. PCA (Principal Component Analysis)
    6. LDA (Latent Dirichlet Allocation)
