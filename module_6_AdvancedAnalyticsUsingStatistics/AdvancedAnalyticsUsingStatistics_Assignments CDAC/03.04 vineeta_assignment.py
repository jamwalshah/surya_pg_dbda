# -*- coding: utf-8 -*-
"""Vineeta_Assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1083bWljjlrxiGgBQ1ZWMuFXljLlP45t0
"""



"""# 1. **Reading the dataset and understanding it**"""

#importing all the library
import numpy as np , pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')

#Exploring the Application Data Sets File
day_df=pd.read_csv("/content/drive/MyDrive/UPGRAD/Linear/day.csv",header=0)
day_df.head()

#checking the number of column in the data set
day_df.shape

#Checking the column data size and data types
day_df.info()

#Checking the statistical data 
day_df.describe()

#Identifying the percentage missing value 
day_df.isna().sum()



"""## **2. Cleaning the data Set and droping the not required columns**"""

#checking the column
day_df.instant



"""Observation: Instant column is just having the index values so we can drop it"""

# droping the instance 
day_df.drop(columns='instant',axis=1,inplace=True)

day_df.head()

# checking column dteday

day_df.dteday

# droping the column 
day_df.drop(columns='dteday',axis=1,inplace=True)

day_df.head(100)

day_df[['casual','registered']]



"""As we can understand the cnt column values are the addition of casual and registered variable
\

"""

# droping the column
day_df.drop(columns=['casual','registered'],axis=1,inplace=True)

day_df.head()



"""### 3. **Data** **Prepration**

#Weathersit column data
1. clear,fewcloud,partialy cloudy
2.Mist+cloudy, Mist+broken clouds,Mist+Few Cloudy,Mist
3. light snow,Light rain+ thunderstrom+Scattered clouds,Light rain+ scattered cloud
4. heavy rain+ ice plalects+ thunderstron+mist, snow+fog
"""

#replacing the value of variable with above mention values

day_df.weathersit.replace({1:"Partialy cloud",2:'Mist_Cloudy',3:'Light_Rain_Scatter',4:'Heavy_Rain_Ice_Pallets_Thunderstrom'},inplace=True)

day_df.head()

"""Other Season column variable
1:"Spring",2:'Summer',3:'falls',4:'Winter'
"""

day_df.season.replace({1:"Spring",2:'Summer',3:'falls',4:'Winter'},inplace=True)
day_df.head()

"""Weekday Variable"""

#counting the number of distinct values are presetn in the weekday variable
day_df.weekday.value_counts()

"""# **Observation**
That the numbers have representation for 0 to 6
- 0: Sunday
- 1: Monday
- 2: Tuesday
- 3: Wednesday
- 4: Thursday
- 5: Friday
- 6: Saturday

"""

day_df.weekday.replace({0:"Sunday",1:'Monday',2:'Tuesday',3:'wednesday',4:'Thrusday',5:'Friday',6:'Saturday'},inplace=True)
day_df.head()

"""# **Visualizing the data**"""

#storing Numeric Values
num_col=day_df.select_dtypes(include=np.number).columns.tolist()[4:]
num_col

#Ploting Pair plot for the data 

sns.pairplot(data=day_df,vars=num_col)
plt.show()

#Checkng the coleration by heatmap between atemp and temp

plt.figure(figsize=(15,10))
sns.heatmap(day_df.corr(),annot=True,cbar_kws={'orientation':'horizontal'})
plt.show()

"""# **Observation**
Its better to drop the temp column and do further modelling on atemp columnn
As the corelation between atemp and temp column is very high 
"""

#dropping the temp variable
day_df.drop(['temp'],axis=1,inplace=True)
day_df.info()

#categorail variable storing
cat_col=day_df.select_dtypes(include=['object']).columns.tolist()
cat_col

plt.figure(figsize=(15,20))
plt.subplot(4,2,1)
plt.suptitle("Categorigal data with target variable")
sns.boxplot(x= cat_col[0],y="cnt",data=day_df)
plt.subplot(4,2,2)
sns.boxplot(x=cat_col[2],y="cnt",data=day_df)
plt.subplot(4,2,3)
sns.boxplot(x=cat_col[1],y="cnt",data=day_df)
plt.subplot(4,2,4)
sns.boxplot(x='holiday',y='cnt',data=day_df)
plt.subplot(4,2,5)
sns.boxplot(x='workingday',y='cnt',data=day_df)
plt.subplot(4,2,6)
sns.boxplot(x='yr',y ='cnt',data=day_df)
plt.subplot(4,2,7)
sns.boxplot(x='mnth',y='cnt',data=day_df)
plt.show()

# Creating Dummy variable

day_df.info()

"""As we can see that the mnth colun is our categorial column so we need to change its data type int64 to object for further processs

other categorial varibales are season, weathersir, weekday,mnth
"""

day_df.mnth=day_df.mnth.astype(object)
day_df.info()

#storing categorical variable in the list
cat_col1=day_df.select_dtypes(include=['object']).columns.tolist()

cat_col1

# creating Dummy Variable of all categorial variable
season_name=pd.get_dummies(day_df[cat_col1[0]],drop_first=True)
month_name=pd.get_dummies(day_df[cat_col1[1]],drop_first=True)
days_name=pd.get_dummies(day_df[cat_col1[2]],drop_first=True)
weather_name=pd.get_dummies(day_df[cat_col1[3]],drop_first=True)
type(season_name)
type(month_name)
type(days_name)
type(weather_name)

#Combinig the data frame

day_df=pd.concat([day_df,season_name,month_name,days_name,weather_name],axis=1)

day_df.drop(cat_col1,axis=1,inplace=True)

day_df.head()

day_df.info()

# Splitting the data into sets for trining and testing

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

np.random.seed(0)

day_train,day_test=train_test_split(day_df,train_size=0.7,test_size=0.3,random_state=100)

day_train.shape

day_test.shape

day_train.head()

#we need to rescale the column

scalers=MinMaxScaler()

n_col=day_df.select_dtypes(include=['int64','float64']).columns.tolist()[3:]

n_col

#fit tranform
day_train[n_col]=scalers.fit_transform(day_train[n_col])
day_train.head()

day_train.describe()

from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression

# creating x and y from The traing data
y_train=day_train.pop('cnt')
x_train=day_train

y_train.head()

lm = LinearRegression()
lm.fit(x_train, y_train)

# REF running

rf= RFE(lm)             
rf= rf.fit(x_train, y_train)

list(zip(x_train.columns,rf.support_,rf.ranking_))

col_name=x_train.columns[rf.support_]
print(col_name)

x_train.columns[~rf.support_]

# Building a model
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor

# creating x_test dataframe with RFE Selected variable
x_train_rf=x_train[col_name]

#Adding the consatnt variable
x_train_rf1=sm.add_constant(x_train_rf)

mod1=sm.OLS(y_train,x_train_rf1).fit()
print(mod1.summary())

# Now we will calculated VIF for the model
def v_nam_model():
  vif=pd.DataFrame()
  x=x_train_rf
  vif['Features']=x.columns
  vif['vif']=[variance_inflation_factor(x.values,i)for i in range (x.shape[1])]
  vif['vif']=round(vif['vif'],2)
  vif=vif.sort_values(by='vif',ascending=False)
  return vif

v_nam_model()

"""Observation: atemp and Hum have very high vif values"""

# drop the hum variable
x_train_rf=x_train_rf.drop(['hum'],axis=1)

#Adding the consatnt variable
x_train_rf2=sm.add_constant(x_train_rf)

mod2=sm.OLS(y_train,x_train_rf2).fit()
print(mod2.summary())

v_nam_model()

#droping Partially cloud

x_train_rf=x_train_rf.drop(['Partialy cloud'],axis=1)

#model 3
#Adding the consatnt variable
x_train_rf3=sm.add_constant(x_train_rf)

mod3=sm.OLS(y_train,x_train_rf3).fit()
print(mod3.summary())

v_nam_model()

#droping the atemp variable 
x_train_rf=x_train_rf.drop(['atemp'],axis=1)

#model 4
#Adding the consatnt variable
x_train_rf4=sm.add_constant(x_train_rf)

mod4=sm.OLS(y_train,x_train_rf4).fit()
print(mod4.summary())

v_nam_model()

#droping the Winspead variable 
x_train_rf=x_train_rf.drop(['windspeed'],axis=1)

#model 5
#Adding the consatnt variable
x_train_rf5=sm.add_constant(x_train_rf)

mod5=sm.OLS(y_train,x_train_rf5).fit()
print(mod5.summary())

#droping the Winter variable 
x_train_rf=x_train_rf.drop(['Winter'],axis=1)

#model 6
#Adding the consatnt variable
x_train_rf6=sm.add_constant(x_train_rf)

mod6=sm.OLS(y_train,x_train_rf6).fit()
print(mod6.summary())

# dropping 10
#droping the variable 
x_train_rf=x_train_rf.drop([10],axis=1)

#model 7
#Adding the consatnt variable
x_train_rf7=sm.add_constant(x_train_rf)

mod7=sm.OLS(y_train,x_train_rf7).fit()
print(mod7.summary())

v_nam_model()

# day variable
day_var=['Monday','Tuesday','Wednesday','Thrusday','Friday','Saturday','Sunday']
day_var

x_train_rf[day_var[0]]=x_train[day_var[0]]
x_train_rf.head()

#model 8
#Adding the consatnt variable
x_train_rf8=sm.add_constant(x_train_rf)

mod8=sm.OLS(y_train,x_train_rf8).fit()
print(mod8.summary())

v_nam_model()

#adding Monday Variable
x_train_rf[day_var[1]]=x_train[day_var[1]]
x_train_rf.head()

#model 9
#Adding the consatnt variable
x_train_rf9=sm.add_constant(x_train_rf)

mod9=sm.OLS(y_train,x_train_rf9).fit()
print(mod9.summary())

v_nam_model()

#droping the variable 
x_train_rf=x_train_rf.drop(day_var[1],axis=1)

#adding Variable
x_train_rf[day_var[3]]=x_train[day_var[3]]
x_train_rf.head()

#model 10
#Adding the consatnt variable
x_train_rf10=sm.add_constant(x_train_rf)

mod10=sm.OLS(y_train,x_train_rf10).fit()
print(mod10.summary())

v_nam_model()

##droping the variable 
x_train_rf=x_train_rf.drop(day_var[3],axis=1)

#adding Variable
x_train_rf[day_var[3]]=x_train[day_var[3]]
x_train_rf.head()

#model 11
#Adding the consatnt variable
x_train_rf11=sm.add_constant(x_train_rf)

mod11=sm.OLS(y_train,x_train_rf11).fit()
print(mod11.summary())

v_nam_model()

#adding Variable
x_train_rf[day_var[5]]=x_train[day_var[5]]
x_train_rf.head()

##droping the variable 
#x_train_rf=x_train_rf.drop(day_var[4],axis=1)

#model 12
#Adding the consatnt variable
x_train_rf12=sm.add_constant(x_train_rf)

mod12=sm.OLS(y_train,x_train_rf12).fit()
print(mod12.summary())

v_nam_model()

##droping the variable 
x_train_rf=x_train_rf.drop(day_var[5],axis=1)

#model 13
#Adding the consatnt variable
x_train_rf13=sm.add_constant(x_train_rf)

mod13=sm.OLS(y_train,x_train_rf13).fit()
print(mod13.summary())

x_train.columns

#Adding variable

#x_train_rf[0]=x_train_rf[0]
#x_train_rf.head()

#model 14
#Adding the consatnt variable
x_train_rf14=sm.add_constant(x_train_rf)

mod14=sm.OLS(y_train,x_train_rf14).fit()
print(mod14.summary())

v_nam_model()

x_train_rf=x_train_rf.drop('falls',axis=1)
x_train_rf=x_train_rf.drop('Tuesday',axis=1)

#Adding variable

x_train_rf[4]=x_train[4]
x_train_rf.head()

x_train_rf=x_train_rf.drop('Tuesday',axis=1)

#model 15
#Adding the consatnt variable
x_train_rf15=sm.add_constant(x_train_rf)

mod15=sm.OLS(y_train,x_train_rf15).fit()
print(mod15.summary())

v_nam_model()

x_train_rf=x_train_rf.drop(4,axis=1)

#Adding variable

x_train_rf[7]=x_train[7]
x_train_rf.head()

#model 16
#Adding the consatnt variable
x_train_rf16=sm.add_constant(x_train_rf)

mod16=sm.OLS(y_train,x_train_rf16).fit()
print(mod16.summary())

v_nam_model()

#Adding variable

x_train_rf[10]=x_train[10]
x_train_rf.head()

#model 17
#Adding the consatnt variable
x_train_rf17=sm.add_constant(x_train_rf)

mod17=sm.OLS(y_train,x_train_rf17).fit()
print(mod17.summary())

#Adding variable

x_train_rf[11]=x_train[11]
x_train_rf.head()

#model 18
#Adding the consatnt variable
x_train_rf18=sm.add_constant(x_train_rf)

mod18=sm.OLS(y_train,x_train_rf18).fit()
print(mod18.summary())

x_train_rf=x_train_rf.drop(11,axis=1)

#Adding the consatnt variable
x_train_rf18=sm.add_constant(x_train_rf)

mod18=sm.OLS(y_train,x_train_rf18).fit()
print(mod18.summary())

#Adding variable

x_train_rf[12]=x_train[12]
x_train_rf.head()

#model 19
#Adding the consatnt variable
x_train_rf19=sm.add_constant(x_train_rf)

mod19=sm.OLS(y_train,x_train_rf19).fit()
print(mod19.summary())

x_train_rf=x_train_rf.drop(12,axis=1)

#Adding the consatnt variable
x_train_rf19=sm.add_constant(x_train_rf)

mod19=sm.OLS(y_train,x_train_rf19).fit()
print(mod19.summary())

v_nam_model()

x_train.columns

# adding variable working day
x_train_rf["workingday"]=x_train["workingday"]
print(x_train_rf.head())

#model 20
#Adding the consatnt variable
x_train_rf20=sm.add_constant(x_train_rf)

mod20=sm.OLS(y_train,x_train_rf20).fit()
print(mod20.summary())

#Droping the variable 

x_train_rf=x_train_rf.drop("workingday",axis=1)

# adding the constant
x_train_rfe20=sm.add_constant(x_train_rf)

# running the model
mod20 = sm.OLS(y_train,x_train_rf20).fit()
print(mod20.summary())

v_nam_model()

"""Model 17 seem to be good, so we can consider that model

"""

print(mod17.summary())

y_train_count=mod17.predict(x_train_rf17)
print(y_train_count)

#calculationg residual
residual=y_train-y_train_count
residual

# ploting the error term

fig = plt.figure(figsize=(10,6))
sns.distplot((residual), bins = 20)
fig.suptitle('Error Terms', fontsize = 20)                
plt.xlabel('Errors', fontsize = 18)

# check tthe shape, columns and residual shape
print(x_train_rf17.shape)
print(x_train_rf17.columns)
print(residual.shape)

#scalling the test data
num_var=['atemp','hum','windspeed','cont']
day_test[num_var]=scalers.fit_transform(day_test[num_var])

x_train1=x_train_rf17.drop(['const'],axis=1)

x_test1=x_train[x_train1.columns]
x_test1=sm.add_constant(x_test1)

x_train_rf17.columns

y_pred=mod17.predict(x_test1)
y_pred

fig = plt.figure(figsize=(10,6))
plt.scatter(y_train,y_pred)
fig.suptitle('Y test Data vs Y prediction Data', fontsize=20)              
plt.xlabel('y_test', fontsize=18)                          
plt.ylabel('y_pred', fontsize=16)

# importing the lib
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score

# calculate the squared of the test and prediction data
np.sqrt(mean_squared_error(y_train, y_pred))

# calculate the r suared from the test and prediction data

r_squ = r2_score(y_train, y_pred)
print(r_squ)

plt.figure(figsize=(10,6))
sns.distplot(residual,kde=True)
plt.title('Normality residuals')

x_train1.head()

print(mod17.summary())

v_nam_model()

print(list(x_train_rf17.columns))

##### **1. According to the model 17. The following are the variables which will help to company to predict the demand increas/decrease.And the demand is depend on these variables.**

#['const', 'yr', 'holiday', 'Summer', 5, 8, 9, 'Mist_Cloudy', 'Monday', 7, 10]



"""**2. The Bikes demand is increase in the following month**

5,8,9,10,yr

**The Bikes demand is decreases in the following time**
holiday, summer,Mist_cloudy
"""

**3.The final equation for the best fitted line is following**

### **cnt = (0.3856 * (yr + 0.2526) * (holiday - 0.0674) * (spring - 0.1904) * (3 + 0.0591) * (5 + 0.1317) * (6 + 0.1632) * (8 + 0.1675) * (9 + 0.1894) * (Mist_Cloudy - 0.0743) * (Monday - 0.0322) * (7 + 0.1323)  * (10 + 0.0898 ))**                                     


##### **4. The recomandation to the comany is, bike demand is higher in the month of March, May, Jun, July, Augest, September and October**